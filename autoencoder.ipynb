{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aromalsr/Thesis/blob/main/autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_eeIpOUAdWs"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib import cm\n",
        "import random\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import mean_squared_error as mse\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, Activation, Dense, Dropout, Softmax, LeakyReLU, BatchNormalization, AveragePooling2D, Reshape, Flatten, Conv2DTranspose\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
        "\n",
        "import glob\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OL12aWSJAdWy"
      },
      "outputs": [],
      "source": [
        "# Size of our input images and no of channels\n",
        "SIZE = 128\n",
        "no_channels = 1\n",
        "\n",
        "#Dataset Path\n",
        "\n",
        "TRAIN_DATASET_PATH = Path(\"/content/enamel_dataset/good_images/GoodImg1\")\n",
        "# VALID_DATASET_PATH = Path(\"/content/enamel_dataset/ValidImg\")\n",
        "PRED1_DATASET_PATH = Path(\"/content/enamel_dataset/pred_goodImg/pred_1\")                      #Dataset without defect\n",
        "PRED2_DATASET_PATH = Path(\"/content/enamel_dataset/blob_defect/blobs\")                       #Dataset with defect\n",
        "PRED3_DATASET_PATH = Path(\"/content/enamel_dataset/scratch_defect/scratches\")                   #Dataset with defect\n",
        "\n",
        "img_paths = list(TRAIN_DATASET_PATH.glob(\"**/*.bmp\"))\n",
        "train_img_paths = img_paths[0:800]\n",
        "valid_img_paths = img_paths[801:1000]\n",
        "pred1_img_paths = list(PRED1_DATASET_PATH.glob(\"**/*.bmp\"))\n",
        "pred2_img_paths = list(PRED2_DATASET_PATH.glob(\"**/*.bmp\"))\n",
        "pred3_img_paths = list(PRED3_DATASET_PATH.glob(\"**/*.bmp\"))\n",
        "#pred4_img_paths = list(PRED2_DATASET_PATH.glob(\"**/*.bmp\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gna4aKJpAdW0"
      },
      "outputs": [],
      "source": [
        "def get_image(img_path):\n",
        "    img_array = []\n",
        "\n",
        "    for img in img_path:\n",
        "\n",
        "        image = cv2.imread(str(img),0)\n",
        "        # image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "        # image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "        #image = cv2.resize(image,(128,128),interpolation=cv2.INTER_AREA)\n",
        "        image = cv2.resize(image, (128, 128))\n",
        "        image = image/255\n",
        "\n",
        "        img_array.append(image)\n",
        "\n",
        "    print('image shape: ', image.shape)\n",
        "\n",
        "    image_np = np.asarray(img_array)\n",
        "    image_np = tf.expand_dims(image_np, axis=-1)\n",
        "    print('shape of input nparray:',image_np.shape)\n",
        "\n",
        "    return image_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8CDVFGlAdW0"
      },
      "outputs": [],
      "source": [
        "print(\"training data:\")\n",
        "x_train = get_image(train_img_paths)\n",
        "print(\"test data:\")\n",
        "x_test = get_image(valid_img_paths)\n",
        "\n",
        "x_train[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0edHJRHrAdW1"
      },
      "outputs": [],
      "source": [
        "x_Train = x_train.numpy()\n",
        "score = ssim(x_Train[100], x_Train[100], full=True, multichannel=True)\n",
        "print(score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ioYE_JlAdW1"
      },
      "outputs": [],
      "source": [
        " random_num = random.randint(0,len(x_train) - 1)\n",
        "#  plt.imshow(x_train[random_num], cmap=\"gray\")\n",
        " x_train_test = tf.squeeze(x_train, axis= -1)\n",
        " print('shape of input nparray:',x_train_test.shape)\n",
        "\n",
        " plt.imshow(x_train_test[random_num], cmap=\"gray\")\n",
        " print(\"image_number = \",random_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTB0qhBUAdW2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create the Encoder and Decoder\n",
        "#Encoder\n",
        "#pass the gray scale input image of size(128,128,1)\n",
        "inputs = tf.keras.Input(shape=(SIZE,SIZE, 1), name='input_layer')\n",
        "# Conv Block 1 -> BatchNorm->leaky Relu\n",
        "encoded = Conv2D(256, kernel_size=3, strides= 1, padding='same', name='conv_1')(inputs)\n",
        "encoded = BatchNormalization(name='batchnorm_1')(encoded)\n",
        "encoded = LeakyReLU(name='relu_1')(encoded)\n",
        "# Conv Block 2 -> BatchNorm->leaky Relu\n",
        "encoded = Conv2D(128, kernel_size=3, strides= 1, padding='same', name='conv_2')(encoded)\n",
        "encoded = BatchNormalization(name='batchnorm_2')(encoded)\n",
        "encoded = LeakyReLU(name='relu_2')(encoded)\n",
        "# Conv Block 3 -> BatchNorm->leaky Relu\n",
        "encoded = Conv2D(64, kernel_size=3, strides= 2, padding='same', name='conv_3')(encoded)\n",
        "encoded = BatchNormalization(name='batchnorm_3')(encoded)\n",
        "encoded = LeakyReLU(name='relu_3')(encoded)\n",
        "# Conv Block 4 -> BatchNorm->leaky Relu\n",
        "encoded = Conv2D(32, kernel_size=3, strides=2, padding='same', name='conv_4')(encoded)\n",
        "encoded = BatchNormalization(name='batchnorm_4')(encoded)\n",
        "encoded = LeakyReLU(name='relu_4')(encoded)\n",
        "# Conv Block 5 -> BatchNorm->leaky Relu\n",
        "encoded = Conv2D(16, kernel_size=3, strides=2, padding='same', name='conv_5')(encoded)\n",
        "encoded = BatchNormalization(name='batchnorm_5')(encoded)\n",
        "encoded = LeakyReLU(name='relu_5')(encoded)\n",
        "\n",
        "#Decoder\n",
        "# DeConv Block 1-> BatchNorm->leaky Relu\n",
        "decoded = Conv2DTranspose(16, 3, strides= 1, padding='same',name='conv_transpose_1')(encoded)\n",
        "decoded = BatchNormalization(name='batchnorm_6')(decoded)\n",
        "decoded = LeakyReLU(name='relu_6')(decoded)\n",
        "# DeConv Block 2-> BatchNorm->leaky Relu\n",
        "decoded = Conv2DTranspose(32, 3, strides= 2, padding='same', name='conv_transpose_2')(decoded)\n",
        "decoded = BatchNormalization(name='batchnorm_7')(decoded)\n",
        "decoded = LeakyReLU(name='relu_7')(decoded)\n",
        "# DeConv Block 3-> BatchNorm->leaky Relu\n",
        "decoded = Conv2DTranspose(64, 3, 2, padding='same', name='conv_transpose_3')(decoded)\n",
        "decoded = BatchNormalization(name='batchnorm_8')(decoded)\n",
        "decoded = LeakyReLU(name='relu_8')(decoded)\n",
        "# DeConv Block 4-> BatchNorm->leaky Relu\n",
        "decoded = Conv2DTranspose(128, 3, 2, padding='same', name='conv_transpose_4')(decoded)\n",
        "decoded = BatchNormalization(name='batchnorm_9')(decoded)\n",
        "decoded = LeakyReLU(name='relu_9')(decoded)\n",
        "\n",
        " # output\n",
        "outputs = Conv2DTranspose(no_channels, 3, 1,padding='same', activation='sigmoid', name='conv_transpose_10')(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IdBSbhqAdW3"
      },
      "outputs": [],
      "source": [
        "def SSIMLoss(y_true, y_pred):\n",
        "   return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred,1.0,filter_size=7))\n",
        "\n",
        "autoencoder = Model(inputs, outputs)\n",
        "autoencoder.compile(optimizer=Adam(lr = 0.001),loss=SSIMLoss, metrics=['mse'])\n",
        "\n",
        "autoencoder.output_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-mm-S8PAdW5"
      },
      "outputs": [],
      "source": [
        "history=autoencoder.fit(x_train, x_train,\n",
        "                epochs=35,\n",
        "                batch_size=32,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test) )\n",
        "\n",
        "# history_2 = autoencoder.fit(\n",
        "#     train_generator,\n",
        "#     steps_per_epoch=500 // batch_size,\n",
        "#     epochs=50,\n",
        "#     validation_data=validation_generator,\n",
        "#     validation_steps=75 // batch_size,\n",
        "#     shuffle=True\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlyuvM_FAdW7"
      },
      "outputs": [],
      "source": [
        "# plot the training and validation accuracy and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBOuK2SOAdW8"
      },
      "source": [
        "## Dataset Without Anomaly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-A4ZXASAdXG"
      },
      "outputs": [],
      "source": [
        "x_pred1 = get_image(pred1_img_paths)\n",
        "x_Pred1 = x_pred1.numpy()                 # not sure | done to solve the error occured with ssim function\n",
        "\n",
        "decoded_imgs_without_anomaly = autoencoder.predict(x_pred1)\n",
        "# decoded_imgs_without_Anomaly = decoded_imgs_without_anomaly.numpy\n",
        "show_result(x_pred1,decoded_imgs_without_anomaly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci1Uff25AdXH"
      },
      "source": [
        "## Dataset With Anomaly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tAWx-diAdXI"
      },
      "outputs": [],
      "source": [
        "x_pred2 = get_image(pred2_img_paths)\n",
        "x_Pred2 = x_pred2.numpy()\n",
        "\n",
        "\n",
        "decoded_imgs_with_blob_anomaly = autoencoder.predict(x_pred2)\n",
        "show_result(x_pred2,decoded_imgs_with_blob_anomaly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVGZqPNdAdXJ"
      },
      "outputs": [],
      "source": [
        "x_pred3 = get_image(pred3_img_paths)\n",
        "x_Pred3 = x_pred3.numpy()\n",
        "\n",
        "\n",
        "decoded_imgs_with_scratch_anomaly = autoencoder.predict(x_pred3)\n",
        "show_result(x_pred3,decoded_imgs_with_scratch_anomaly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lheo79tfAdXP"
      },
      "source": [
        "## Calculating SSIM Loss for each images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNaCmyopAdXP"
      },
      "outputs": [],
      "source": [
        "# ssim_with_anomaly_array = np.asarray([ssim(x_Pred2[i], decoded_imgs_with_anomoly[i], multichannel = True) for i in range(0, x_Pred2.shape[0])]).reshape(-1,1)\n",
        "ssim_with_blob_anomaly_array = np.asarray([ssim(x_Pred2[i], decoded_imgs_with_blob_anomaly[i], multichannel = True) for i in range(0, x_Pred2.shape[0])]).reshape(-1,1)\n",
        "ssim_with_scratch_anomaly_array = np.asarray([ssim(x_Pred3[i], decoded_imgs_with_scratch_anomaly[i], multichannel = True) for i in range(0, x_Pred3.shape[0])]).reshape(-1,1)\n",
        "ssim_without_anomaly_array = np.asarray([ssim(x_Pred1[i], decoded_imgs_without_anomaly[i], multichannel = True) for i in range(0, x_Pred1.shape[0])]).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRL5q7KOAdXR"
      },
      "source": [
        "### SSIM for With & Without Anomaly Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdo6FgMUAdXS"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize= (10,7))\n",
        "plt.title(\"SSIM\")\n",
        "# plt.scatter(x=np.arange(0,len(ssim_with_anomaly_array)),y=ssim_with_anomaly_array, label='ssim with anomaly')\n",
        "plt.scatter(x=np.arange(0,len(ssim_with_blob_anomaly_array)),y=ssim_with_blob_anomaly_array, label='ssim with blob anomaly')\n",
        "plt.scatter(x=np.arange(0,len(ssim_with_scratch_anomaly_array)),y=ssim_with_scratch_anomaly_array, label='ssim with scratch anomaly')\n",
        "plt.scatter(x=np.arange(0,len(ssim_without_anomaly_array)),y=ssim_without_anomaly_array, label='ssim without anomaly')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv68kzPNAdXV"
      },
      "source": [
        "## Squeezed Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jDyLaZdAdXW"
      },
      "outputs": [],
      "source": [
        "def get_error_ssim(imageA, imageB):\n",
        "\n",
        "    fragmented_image_error = []\n",
        "    window = 20\n",
        "    temp_error = 1\n",
        "\n",
        "    for x in range(0, imageA.shape[0]-window+1, int(window/2)):\n",
        "        for y in range(0, imageA.shape[1]-window+1, int(window/2)):\n",
        "            error = ssim(imageA[x : x + window, y : y + window],\n",
        "                        imageB[x : x + window, y : y + window])\n",
        "            fragmented_image_error.append(error)\n",
        "            if error < temp_error:\n",
        "                pos = (x,y)\n",
        "                temp_error = error\n",
        "\n",
        "    return temp_error, pos, window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mdVx01nAdXX"
      },
      "outputs": [],
      "source": [
        "X_pred1=np.squeeze(x_pred1)\n",
        "Decoded_imgs_without_anomaly = np.squeeze(decoded_imgs_without_anomaly)\n",
        "\n",
        "# X_pred2 = np.squeeze(x_pred2)\n",
        "# Decoded_imgs_with_anomaly=np.squeeze(decoded_imgs_with_anomaly)\n",
        "\n",
        "X_pred2 = np.squeeze(x_pred2)\n",
        "Decoded_imgs_with_blob_anomaly=np.squeeze(decoded_imgs_with_blob_anomaly)\n",
        "X_pred3 = np.squeeze(x_pred3)\n",
        "Decoded_imgs_with_scratch_anomaly=np.squeeze(decoded_imgs_with_scratch_anomaly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztk9C7JQAdXX"
      },
      "outputs": [],
      "source": [
        "print(X_pred1.shape)\n",
        "print(Decoded_imgs_without_anomaly.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2PJfxVxAdXY"
      },
      "outputs": [],
      "source": [
        "ssim_with_blob_anomaly_arr_window = np.asarray([get_error_ssim(X_pred2[i],Decoded_imgs_with_blob_anomaly[i]) for i in range(0, X_pred2.shape[0])])\n",
        "ssim_with_scratch_anomaly_arr_window = np.asarray([get_error_ssim(X_pred3[i],Decoded_imgs_with_scratch_anomaly[i]) for i in range(0, X_pred3.shape[0])])\n",
        "ssim_without_anomaly_arr_window = np.asarray([get_error_ssim(X_pred1[i],Decoded_imgs_without_anomaly [i]) for i in range(0, X_pred1.shape[0])])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A22HNQmcAdXZ"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.title(\"SSIM\")\n",
        "# plt.scatter(x=np.arange(0,len(ssim_with_anomaly_arr_window)),y=ssim_with_anomaly_arr_window, label='ssim with anomaly')\n",
        "plt.scatter(x=np.arange(0,len(ssim_with_blob_anomaly_arr_window)),y=ssim_with_blob_anomaly_arr_window[:,0], label='ssim with blob anomaly')\n",
        "plt.scatter(x=np.arange(0,len(ssim_with_scratch_anomaly_arr_window)),y=ssim_with_scratch_anomaly_arr_window[:,0], label='ssim with scratch anomaly')\n",
        "plt.scatter(x=np.arange(0,len(ssim_without_anomaly_arr_window)),y=ssim_without_anomaly_arr_window[:,0], label='ssim without anomaly')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}